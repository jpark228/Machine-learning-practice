{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter6_TitanicClassifier",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu1uVq9Oiq39"
      },
      "source": [
        "####Installing Autokeras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFHQOmXoSYbo"
      },
      "source": [
        "pip install autokeras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3IgqZhAUohb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import autokeras as ak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuL-84KSSYbu"
      },
      "source": [
        "#### Getting the datasets \n",
        "In this example, we download the Titanic dataset files and pass the dowloaded file path to the AutoKeras classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NloyCKvoSYbv"
      },
      "source": [
        "train_file_url = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
        "test_file_url = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM7Pl76ZQ5E0"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(train_file_url)\n",
        "test_df = pd.read_csv(test_file_url)\n",
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HyAJ-WWg8UQ"
      },
      "source": [
        "Now we prepare the data for training by dividing the pandas data frame into sets of inputs and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nqKtlHo2rw6"
      },
      "source": [
        "x_train_df, y_train_df = train_df.drop(['survived'], axis=1), train_df['survived']\n",
        "x_test_df, y_test_df = test_df.drop(['survived'], axis=1), test_df['survived']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CgvSBr993W1"
      },
      "source": [
        "### Creating and training the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chWQb7AVSkTe"
      },
      "source": [
        "\n",
        "# Initialize the StructuredDataClassifier\n",
        "clf = ak.StructuredDataClassifier(\n",
        "    max_trials=2,\n",
        "    overwrite=True,\n",
        ")\n",
        "\n",
        "# Search for the best model.\n",
        "clf.fit(\n",
        "    x_train_df,\n",
        "    y_train_df,\n",
        "    epochs=10,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q2-S2R0m2CL"
      },
      "source": [
        "'''\n",
        "# We can also pass the csv url directly to our model without create a pandas dataframe\n",
        "clf = ak.StructuredDataClassifier(\n",
        "    max_trials=2,\n",
        "    overwrite=True,\n",
        ")\n",
        "\n",
        "clf.fit(\n",
        "    train_file_url,    # csv url\n",
        "    'survived',        # target column name\n",
        "    epochs=10,\n",
        ")\n",
        "print(clf.evaluate(test_file_url, 'survived'))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BF5OpQr-IEf"
      },
      "source": [
        "### Evaluating the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkoBzlfIWgAp"
      },
      "source": [
        "clf.evaluate(x_test_df, y_test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ThKQ5GvI193"
      },
      "source": [
        "###Visualizing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jokSoUSCN01Y"
      },
      "source": [
        "# First we export the model to a keras model\n",
        "keras_model = clf.export_model()\n",
        "\n",
        "# Now, we ask for the model Sumary:\n",
        "keras_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0UJsat3N387"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(keras_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}