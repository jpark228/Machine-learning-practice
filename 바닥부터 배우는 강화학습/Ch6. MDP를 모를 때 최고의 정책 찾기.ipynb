{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ch6. MDP를 모를 때 최고의 정책 찾기.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMYrP8wI7ImuMcxhcB+h297"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 몬테카를로 컨트롤"],"metadata":{"id":"90P2JHz8o5DM"}},{"cell_type":"code","source":["import random\n","import numpy as np"],"metadata":{"id":"RhuR9UWGo7d0","executionInfo":{"status":"ok","timestamp":1653053572784,"user_tz":-540,"elapsed":2,"user":{"displayName":"박준규","userId":"16577167999398926509"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["class GridWorld():\n","    def __init__(self):\n","        self.x=0\n","        self.y=0\n","    \n","    def step(self, a):\n","        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n","        if a==0:\n","            self.move_left()\n","        elif a==1:\n","            self.move_up()\n","        elif a==2:\n","            self.move_right()\n","        elif a==3:\n","            self.move_down()\n","\n","        reward = -1 # 보상은 항상 -1로 고정\n","        done = self.is_done()\n","        return (self.x, self.y), reward, done\n","\n","    def move_left(self):\n","        if self.y==0:\n","            pass\n","        elif self.y==3 and self.x in [0,1,2]:\n","            pass\n","        elif self.y==5 and self.x in [2,3,4]:\n","            pass\n","        else:\n","            self.y -= 1\n","\n","    def move_right(self):\n","        if self.y==1 and self.x in [0,1,2]:\n","            pass\n","        elif self.y==3 and self.x in [2,3,4]:\n","            pass\n","        elif self.y==6:\n","            pass\n","        else:\n","            self.y += 1\n","      \n","    def move_up(self):\n","        if self.x==0:\n","            pass\n","        elif self.x==3 and self.y==2:\n","            pass\n","        else:\n","            self.x -= 1\n","\n","    def move_down(self):\n","        if self.x==4:\n","            pass\n","        elif self.x==1 and self.y==4:\n","            pass\n","        else:\n","            self.x+=1\n","\n","    def is_done(self):\n","        if self.x==4 and self.y==6:\n","            return True\n","        else:\n","            return False\n","      \n","    def reset(self):\n","        self.x = 0\n","        self.y = 0\n","        return (self.x, self.y)"],"metadata":{"id":"AvQbDqFzq5zd","executionInfo":{"status":"ok","timestamp":1653053583404,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준규","userId":"16577167999398926509"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class QAgent():\n","    def __init__(self):\n","        self.q_table = np.zeros((5, 7, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n","        self.eps = 0.9\n","\n","    def select_action(self, s):\n","        # eps-greedy로 액션을 선택해준다\n","        x, y = s\n","        coin = random.random()\n","        if coin < self.eps:\n","            action = random.randint(0,3)\n","        else:\n","            action_val = self.q_table[x,y,:]\n","            action = np.argmax(action_val)\n","        return action\n","\n","    def update_table(self, transition):\n","        s, a, r, s_prime = transition\n","        x,y = s\n","        next_x, next_y = s_prime\n","        a_prime = self.select_action(s_prime) # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n","        # Q러닝 업데이트 식을 이용 \n","        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + np.amax(self.q_table[next_x,next_y,:]) - self.q_table[x,y,a])\n","\n","    def anneal_eps(self):\n","        self.eps -= 0.01  # Q러닝에선 epsilon 이 좀더 천천히 줄어 들도록 함.\n","        self.eps = max(self.eps, 0.2) \n","\n","    def show_table(self):\n","        q_lst = self.q_table.tolist()\n","        data = np.zeros((5,7))\n","        for row_idx in range(len(q_lst)):\n","            row = q_lst[row_idx]\n","            for col_idx in range(len(row)):\n","                col = row[col_idx]\n","                action = np.argmax(col)\n","                data[row_idx, col_idx] = action\n","        print(data)"],"metadata":{"id":"Sg440oGxq-js","executionInfo":{"status":"ok","timestamp":1653053694926,"user_tz":-540,"elapsed":312,"user":{"displayName":"박준규","userId":"16577167999398926509"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def main():\n","    env = GridWorld()\n","    agent = QAgent()\n","\n","    for n_epi in range(1000):\n","        done = False\n","\n","        s = env.reset()\n","        while not done:\n","            a = agent.select_action(s)\n","            s_prime, r, done = env.step(a)\n","            agent.update_table((s,a,r,s_prime))\n","            s = s_prime\n","        agent.anneal_eps()\n","\n","    agent.show_table()"],"metadata":{"id":"fVgmnx59raAD","executionInfo":{"status":"ok","timestamp":1653053831849,"user_tz":-540,"elapsed":297,"user":{"displayName":"박준규","userId":"16577167999398926509"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RrAk94Gir7a0","executionInfo":{"status":"ok","timestamp":1653053915006,"user_tz":-540,"elapsed":743,"user":{"displayName":"박준규","userId":"16577167999398926509"}},"outputId":"ab2e6f4a-d800-494f-cb7a-cd913131ac45"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[3. 3. 0. 2. 2. 2. 3.]\n"," [3. 3. 0. 2. 2. 3. 3.]\n"," [2. 3. 0. 1. 0. 2. 3.]\n"," [2. 2. 2. 1. 0. 2. 3.]\n"," [2. 1. 1. 1. 0. 2. 0.]]\n"]}]},{"cell_type":"markdown","source":["# TD 컨트롤 1 - SARSA"],"metadata":{"id":"2USI5qIpsQhl"}},{"cell_type":"code","source":["class QAgent():\n","    def __init__(self):\n","        self.q_table = np.zeros((5, 7, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n","        self.eps = 0.9\n","\n","    def select_action(self, s):\n","        # eps-greedy로 액션을 선택해준다\n","        x, y = s\n","        coin = random.random()\n","        if coin < self.eps:\n","            action = random.randint(0,3)\n","        else:\n","            action_val = self.q_table[x,y,:]\n","            action = np.argmax(action_val)\n","        return action\n","\n","    def update_table(self, transition):\n","        s, a, r, s_prime = transition\n","        x,y = s\n","        next_x, next_y = s_prime\n","        a_prime = self.select_action(s_prime) # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n","        # SARSA 업데이트 식을 이용\n","        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + self.q_table[next_x,next_y,a_prime] - self.q_table[x,y,a])\n","\n","    def anneal_eps(self):\n","        self.eps -= 0.03\n","        self.eps = max(self.eps, 0.1)\n","\n","    def show_table(self):\n","        q_lst = self.q_table.tolist()\n","        data = np.zeros((5,7))\n","        for row_idx in range(len(q_lst)):\n","            row = q_lst[row_idx]\n","            for col_idx in range(len(row)):\n","                col = row[col_idx]\n","                action = np.argmax(col)\n","                data[row_idx, col_idx] = action\n","        print(data)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2UDkeYFtc3t","executionInfo":{"status":"ok","timestamp":1653054305169,"user_tz":-540,"elapsed":302,"user":{"displayName":"박준규","userId":"16577167999398926509"}},"outputId":"d6122ee2-5758-4e07-8248-73508d22da47"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[3. 2. 0. 2. 2. 1. 3.]\n"," [3. 3. 0. 2. 2. 3. 3.]\n"," [3. 3. 0. 1. 0. 3. 3.]\n"," [2. 2. 2. 1. 0. 3. 3.]\n"," [2. 3. 1. 1. 0. 2. 0.]]\n"]}]},{"cell_type":"markdown","source":["# TD 컨트롤 2 - Q러닝"],"metadata":{"id":"v0-FMz58t3lG"}},{"cell_type":"code","source":["class QAgent():\n","    def __init__(self):\n","        self.q_table = np.zeros((5, 7, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n","        self.eps = 0.9\n","\n","    def select_action(self, s):\n","        # eps-greedy로 액션을 선택해준다\n","        x, y = s\n","        coin = random.random()\n","        if coin < self.eps:\n","            action = random.randint(0,3)\n","        else:\n","            action_val = self.q_table[x,y,:]\n","            action = np.argmax(action_val)\n","        return action\n","\n","    def update_table(self, transition):\n","        s, a, r, s_prime = transition\n","        x,y = s\n","        next_x, next_y = s_prime\n","        a_prime = self.select_action(s_prime) # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n","        # Q러닝 업데이트 식을 이용 \n","        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + np.amax(self.q_table[next_x,next_y,:]) - self.q_table[x,y,a])\n","\n","    def anneal_eps(self):\n","        self.eps -= 0.01  # Q러닝에선 epsilon 이 좀더 천천히 줄어 들도록 함.\n","        self.eps = max(self.eps, 0.2) \n","\n","    def show_table(self):\n","        q_lst = self.q_table.tolist()\n","        data = np.zeros((5,7))\n","        for row_idx in range(len(q_lst)):\n","            row = q_lst[row_idx]\n","            for col_idx in range(len(row)):\n","                col = row[col_idx]\n","                action = np.argmax(col)\n","                data[row_idx, col_idx] = action\n","        print(data)\n","      \n","\n","def main():\n","    env = GridWorld()\n","    agent = QAgent()\n","\n","    for n_epi in range(1000):\n","        done = False\n","\n","        s = env.reset()\n","        while not done:\n","            a = agent.select_action(s)\n","            s_prime, r, done = env.step(a)\n","            agent.update_table((s,a,r,s_prime))\n","            s = s_prime\n","        agent.anneal_eps()\n","\n","    agent.show_table()\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNP_FGaEuRBV","executionInfo":{"status":"ok","timestamp":1653055647243,"user_tz":-540,"elapsed":682,"user":{"displayName":"박준규","userId":"16577167999398926509"}},"outputId":"d8bc784e-53fd-4ee3-becb-921dc1924eba"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2. 3. 0. 2. 2. 3. 3.]\n"," [3. 3. 0. 2. 2. 2. 3.]\n"," [2. 3. 0. 1. 0. 2. 3.]\n"," [2. 2. 2. 1. 0. 3. 3.]\n"," [3. 1. 1. 1. 0. 2. 0.]]\n"]}]}]}