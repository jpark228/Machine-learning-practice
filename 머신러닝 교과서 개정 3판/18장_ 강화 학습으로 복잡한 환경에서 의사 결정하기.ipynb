{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aW0h86w9-C2o"},"outputs":[],"source":["from IPython.display import Image"]},{"cell_type":"markdown","metadata":{"id":"BjkzSp0e-C2o"},"source":["# 경험에서 배웁니다\n","\n","## 강화 학습 이해하기\n","\n","## 강화 학습 시스템의 에이전트-환경 인터페이스 정의하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446},"id":"he9yqXQY-C2p","outputId":"9d5e58ed-ce15-42f4-8ff0-0b8585e05ecd"},"outputs":[{"data":{"text/html":["<img src=\"https://git.io/JtTQo\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://git.io/JtTQo', width=700)"]},{"cell_type":"markdown","metadata":{"id":"ULH0eSrQ-C2p"},"source":["# 강화 학습의 기초 이론\n","\n","## 마르코프 결정 과정\n","\n","## 마르코프 결정 과정의 수학 공식"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"id":"QXU39HEE-C2q","outputId":"c0b38b6a-8404-49cf-98bc-db50a31538e2"},"outputs":[{"data":{"text/html":["<img src=\"https://git.io/JtTQi\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://git.io/JtTQi', width=700)"]},{"cell_type":"markdown","metadata":{"id":"wVy6MQKE-C2q"},"source":["### 마르코프 과정 시각화"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"R4u_4d6w-C2q","outputId":"2748c195-9c5f-49b9-9b7f-de6d068cf2f1"},"outputs":[{"data":{"text/html":["<img src=\"https://git.io/JtTQP\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://git.io/JtTQP', width=700)"]},{"cell_type":"markdown","metadata":{"id":"xN6MVEs3-C2q"},"source":["### 에피소드 작업 대 연속적인 작업"]},{"cell_type":"markdown","metadata":{"id":"FdueKfC5-C2r"},"source":["## 강화 학습 용어: 대가, 정책, 가치 함수\n","\n","### 대가"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"id":"k0i6lJUZ-C2r","outputId":"9191f6ce-0ce1-4de7-99fe-97a33457f7dc"},"outputs":[{"data":{"text/html":["<img src=\"https://git.io/Jtkcl\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# 할인 계수에 대하여\n","\n","Image(url='https://git.io/Jtkcl', width=700)"]},{"cell_type":"markdown","metadata":{"id":"BS8vmyz7-C2r"},"source":["### 정책\n","\n","### 가치 함수"]},{"cell_type":"markdown","metadata":{"id":"jzQGkggx-C2r"},"source":["## 벨먼 방정식을 사용한 동적 계획법"]},{"cell_type":"markdown","metadata":{"id":"mOwyDK-f-C2r"},"source":["# 강화 학습 알고리즘"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"fXKrwBqo-C2r","outputId":"1bc3dc9a-c6cb-4774-e7c8-623343dbc0b3"},"outputs":[{"data":{"text/html":["<img src=\"https://git.io/Jtkc4\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://git.io/Jtkc4', width=700)"]},{"cell_type":"markdown","metadata":{"id":"diXYfr-1-C2s"},"source":["## 동적 계획법\n","\n","### 정책 평가 - 동적 계획법으로 가치 함수 예측하기\n","\n","### 추정된 가치 함수로 정책 향상시키기\n","\n","### 정책 반복\n","\n","### 가치 반복"]},{"cell_type":"markdown","metadata":{"id":"f8zZTfDr-C2s"},"source":["## 몬테 카를로를 사용한 강화 학습\n","\n","### MC를 사용한 상태-가치 함수 추정\n","\n","### MC를 사용한 행동-가치 함수 추정\n","\n","### MC 제어를 사용해 최적의 정책 찾기\n","\n","### 정책 향상 - 행동-가치 함수로부터 그리디 정책 계산하기"]},{"cell_type":"markdown","metadata":{"id":"MYHcYa2n-C2s"},"source":["## 시간차 학습\n","\n","### TD 예측\n","\n","### 온-폴리시 TD 제어 (SARSA)\n","\n","### 오프-폴리시 TD 제어 (Q-러닝)"]},{"cell_type":"markdown","metadata":{"id":"6r2LUtd1-C2s"},"source":["# 첫 번째 강화 학습 알고리즘 구현하기\n","\n","## OpenAI 짐 툴킷 소개\n","\n","### OpenAI 짐에 포함된 환경 사용하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"_hD-JWTu-C2s","outputId":"30dd4d93-3773-4081-d8d3-8c6221528a81"},"outputs":[{"data":{"text/html":["<img src=\"https://git.io/JtkcB\" width=\"800\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://git.io/JtkcB', width=800)"]},{"cell_type":"markdown","metadata":{"id":"LeF3J9WX-C2s"},"source":["### 그리드 월드"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":303},"id":"YxGS_WO4-C2s","outputId":"5f859cc4-d9f3-4406-c8d9-c6e88df4c2b6"},"outputs":[{"data":{"text/html":["<img src=\"https://git.io/Jtkc0\" width=\"800\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://git.io/Jtkc0', width=800)"]},{"cell_type":"markdown","metadata":{"id":"SIehs9EQ-C2t"},"source":["### OpenAI 짐에서 그리드 월드 환경 구현하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":563},"id":"A7V3AiWN-C2u","outputId":"69083d2e-28e8-4d1f-a6e8-85992bc33d89"},"outputs":[{"data":{"text/html":["<img src=\"https://bit.ly/34MpM2p\" width=\"600\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://bit.ly/34MpM2p', width=600)"]},{"cell_type":"markdown","metadata":{"id":"q8uvcoXy-C2v"},"source":["## Q-러닝으로 그리드 월드 문제 풀기\n","\n","### Q-러닝 알고리즘 구현하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":593},"id":"n9Xtr4Kj-C2v","outputId":"c0051696-502a-418b-879e-a9db76bcdda5"},"outputs":[{"data":{"text/html":["<img src=\"https://bit.ly/2TBkxR3\" width=\"800\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://bit.ly/2TBkxR3', width=800)"]},{"cell_type":"markdown","metadata":{"id":"f9v2Yd6B-C2w"},"source":["## 심층 Q-러닝"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"kpGLb-g3-C2w","outputId":"ffc57026-3a7f-418e-97ca-fc52cad4fbb7"},"outputs":[{"data":{"text/html":["<img src=\"https://bit.ly/3yUaFlv\" width=\"800\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://bit.ly/3yUaFlv', width=800)"]},{"cell_type":"markdown","metadata":{"id":"YnUKSG3z-C2w"},"source":["### Q-러닝 알고리즘에 따라 DQN 모델 훈련하기\n","\n","#### 재생 메모리"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"FLp8-kCD-C2w","outputId":"68ab49a7-dbf2-49d7-8d39-0e0bf6ee9f63"},"outputs":[{"data":{"text/html":["<img src=\"https://bit.ly/34CELfz\" width=\"800\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://bit.ly/34CELfz', width=800)"]},{"cell_type":"markdown","metadata":{"id":"_Z8jBIMt-C2w"},"source":["#### 손실 계산을 위해 타깃 가치 결정하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":538},"id":"YsFDayA6-C2w","outputId":"8b33254f-a1ef-44e3-9590-49bd132f8e46"},"outputs":[{"data":{"text/html":["<img src=\"https://bit.ly/34Fkwhb\" width=\"800\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://bit.ly/34Fkwhb', width=800)"]},{"cell_type":"markdown","metadata":{"id":"KUTwBa5Q-C2x"},"source":["## 심층 Q-러닝 알고리즘 구현"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"-rTBIyHU-C2x","outputId":"4499bc7f-22d1-40b3-a3e3-8e62e5079823"},"outputs":[{"data":{"text/html":["<img src=\"https://bit.ly/2TDralR\" width=\"800\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["Image(url='https://bit.ly/2TDralR', width=800)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"18장: 강화 학습으로 복잡한 환경에서 의사 결정하기.ipynb","provenance":[{"file_id":"https://github.com/rickiepark/python-machine-learning-book-3rd-edition/blob/master/ch18/ch18.ipynb","timestamp":1652117391314}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}